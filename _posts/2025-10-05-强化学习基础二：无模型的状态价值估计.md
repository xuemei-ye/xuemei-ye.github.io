---
title: 强化学习基础二：无模型的状态价值估计|MC & TD
date: 2025-10-05 15:00:00 +0800
#img_path: ../assets/img/posts/2025-10-04-强化学习基础一：如何用迭代法求解贝尔曼方程？
categories:
  - 强化学习
math: true
mermaid: true
toc: true
---
注意到，前面介绍的迭代法中使用到了 p(r|s,a)，这是关于环境的信息，包含了完备的状态转移和动作-奖励的概率分布，是基于模型的算法（model-based），但实际问题中，很多时候并不能拿到完备的概率分布，对于这个问题，发展出两种方式，一种是先用环境相关的数据估计状态-动作和奖励间的概率分布，在基于这个模型来迭代策略，一种是 model-free 的算法，model-free 相对更加实用，蒙特卡洛和时间差分是model-free算法中的两种代表性思路。
## 蒙特卡洛算法

蒙特卡洛算法（monte-carlon）算法是将迭代法用于无模型的情况中，value-iteration 和 policy-iteration 核心都是求解 q(s,a)，所以接下来的问题是，如何在 model-free 的情况下估计 q(s,a)？

结合动作值函数的定义和大数定律：

![微信截图_20251003181852.png](../assets/img/posts/2025-10-04-强化学习基础一：如何用迭代法求解贝尔曼方程？/微信截图_20251003181852.png)

![微信截图_20251003182140.png](../assets/img/posts/2025-10-04-强化学习基础一：如何用迭代法求解贝尔曼方程？/微信截图_20251003182140.png){: width="600" height="500" }


得出用经验数据来估计动作值函数的方式：

![微信截图_20251003182426.png](../assets/img/posts/2025-10-04-强化学习基础一：如何用迭代法求解贝尔曼方程？/微信截图_20251003182426.png)

- 这里的 g(s,a)是 $G_t(s,a)$ 的一个采样，是在时刻t，从状态 s，采取动作 a 后，运行 episode 结束的一条轨迹的累积回报。
- 根据大数定律，当采样轨迹足够多时，对所有轨迹的累积回报求均值，会接近期望回报。
- 有模型的时候，可以根据模型求解，没有模型的时候，用经验数据来估计期望回报。

MC算法步骤：

![微信截图_20251003184218.png](../assets/img/posts/2025-10-04-强化学习基础一：如何用迭代法求解贝尔曼方程？/微信截图_20251003184218.png){: width="600" height="500" }

- 跟 policy-iteration 相比，区别在于策略评估时，用经验数据来估计动作值函数，而不是基于模型来求解贝尔曼方程中的 v(s)。
- 这个算法是MC的基本算法，因为对于每一个状态-动作对，都需要收集从该状态到结束的轨迹，数据使用效率很低，提高数据使用效率，一个 episode 截断多次使用，重复使用一条轨迹的数据来计算轨迹中所有经过的状态-动作对，为了提高计算和更新的效率，可以在每一条轨迹结束后立即更新，而不用等到所有轨迹收集完毕。

## 时间差分学习

蒙特卡洛方法至少需要等到整个 episode 运行完得到累积回报后，才能更新，早期状态获取反馈有延迟，不同轨迹之间累积回报波动较大，加上多步奖励积累放大了波动，导致价值估计方差很大，收敛速度慢。时序差分（Temporal Differences）方法能够在每个 step 之后使用即时奖励更新估计值，结合了蒙特卡洛无模型和迭代法即时更新的优点，大大提高收敛速度。

TD方法是由 Richard S. Sutton在论文 Learning to Predict by the Methods of Temporal Differences 中提出的，其核心思想与心理学和动物学习中的 Rescorla-Wagner 模型 类似，Rescorla-Wagner模型是指，**实际发生的事情和心理预测的情况差异越大，人和动物越能从这种预测误差中学习**。这跟我们在生活中的体验很接近，预测误差越大，信息量越大，之前认知中可调整的部分越大。

TD 方法的核心便在于如何使用即时奖励来计算预测误差，以及如何用预测误差来更新之前的估计值。

理论上，预测误差由当前估计值 v(s) 和实际的 state-value 来计算，但 state-value 是未知的，且正是需要估计的目标值。所以这里引入即时奖励，用即时奖励和当前价值函数对下一个状态价值的估计 v(s’) 来作为更新目标，进而计算预测误差，然后用自举法来更新当前估计值。数学表示如下：

![20251004161649.png](../assets/img/posts/2025-10-04-强化学习基础一：如何用迭代法求解贝尔曼方程？/20251004161649.png)

- 只更新当前时刻 t 访问的状态值。
- 更新方式类似于梯度下降，区别在于梯度下降的更新目标是固定的，而 TD 方法中，更新目标随着价值函数的更新而更新。
- 即时奖励作为信号，引导价值函数朝着更阶级实际奖励的方向更新。

公式中各部分名称：

![20251004162422.png](../assets/img/posts/2025-10-04-强化学习基础一：如何用迭代法求解贝尔曼方程？/20251004162422.png)

- 能够从数学上证明，$\bar{v_t}$ 比 $v_t$ 更接近 v_π，当 td-error 逼近零时，$v_t$ 逼近 $v_π$

在数学上，TD算法实际上是在无模型的条件下，求解给定策略的贝尔曼方程：

![20251005141551.png](../assets/img/posts/2025-10-04-强化学习基础一：如何用迭代法求解贝尔曼方程？/20251005141551.png)

- TD算法是由 Robbins-Monro algorithm 求解贝尔曼方程推导得出。（RM算法是随机近似算法的一种，随机梯度下降就是RM算法的一种特殊形式。）
