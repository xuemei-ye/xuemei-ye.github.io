---
title: 强化学习基础一：如何用迭代法求解贝尔曼方程？
date: 2025-10-04 01:00:00 +0800
categories:
  - 强化学习
math: true
mermaid: true
toc: true
---

强化学习基础系列博客从强化学习最基本的概念说起，梳理强化学习是怎么从最简单的q-table演变到近年来常用的ddpg、ppo等算法，描绘出各个算法之间的脉络和内在关联，展现各个算法面临的问题，以及在基础上延伸出的新算法，希望能让读者看到强化学习面临的问题，算法演进的全貌和动力。这些文章避开强化学习诸多概念定义，只介绍少量必要概念，更多的集中于核心算法的演变思路。

# 强化学习基本概念|MDP

强化学习这个概念来源于行为心理学中的“效果律”（试错法），直观地看，人跟环境交互，某些行为会带来奖励，某些行为带来惩罚，人会加强带来奖励的行为，减弱带来惩罚的行为。强化学习的基本思路也是如此，根据环境的反馈，不断优化行为，使得最终的奖励最大化。

强化学习的核心问题便集中在这个模式之中，比如如何构建模拟环境，能够更真实的模拟现实环境中的状态和反馈，进而训练能够应用到现实环境中；以及如何构建奖励，如何设计动作，如何优化策略迭代训练。

举个例子，我们想训练能够进行五子棋对弈的智能体，需要将双方棋子的位置和空点相关的盘面信息抽象为状态特征，而奖励的设计可以是最终获胜为正得分，输了就为负得分，平局为零分，也可以设计一些中间奖励，比如连成双三或者死四也给正向奖励。动作则是在棋盘空点坐标上放我方棋子。

这里边提到的状态、奖励、动作、下一个状态，被称为四元组，符号表示为（s、r、a、s‘）（state、reward、action、next_state）从起始状态开始，不断从动作集中选择动作，状态不断转移，形成一个序列或轨迹：S0,A0,R1,S1,A1,R2,S2,A2,R3….

为了从数学上对这个过程进行表述，将其抽象为马尔科夫决策过程（MDP），mdp是描述在随机环境中序列决策的数学框架。包含四元组元素和策略及状态转移，马尔科夫属性是指后续状态分布只与当前状态有关，与过去状态无关。比如棋盘游戏中，你做决策只需要知道当前状态，而并不需要知道过去每一步是怎么走过来的，也就是当前状态其实已经包含了过去所有决策的信息。

在不同状态下选择不同动作，形成不同策略，策略分为确定性策略和随机策略，确定性策略是指在某个状态下选择某个动作的概率为1，所以确定性策略下，在环境中运行多次，生产的轨迹是确定且相同的。随机策略是指在某个状态下选择动作是一个概率分布，在环境中用随机策略运行多次，会生成不同的轨迹。

强化学习的核心主题就是在这样的状态和动作定义下，如何选择动作，进而最大化期望累积回报。奖励是在单次动作的意义上，累积回报（return）也就是从开始到结束所有动作获取奖励累加，期望累积回报者是指在某个策略下，生成的多条轨迹的累积回报的均值。

在相对复杂的情况中，以一定概率选择动作之后，转移到的状态和获取的奖励都是一个概率分布，这时算期望回报，就需要将到选择动作、转移到不同状态，获取不同奖励的概率分布考虑在内。

评估一个策略的好坏的有两种方法，一种是遍历该策略生成的所有可能的轨迹路线，来计算累积回报均值，一种是根据策略的概率分布和状态转移概率、获取的奖励回报概率，来计算期望累积回报。

策略评估是从初始状态到终态的期望回报，而动作选择是在每一个状态下做出的，所以将状态的价值定义为从该状态到终态的期望回报，在选择动作时，选择能够转移到更高价值状态的确定动作或者动作概率分布即可。

